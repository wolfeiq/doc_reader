

#rewrote the original script generated by Claude and split into multiple files

import argparse
import asyncio
import logging
from pathlib import Path
from typing import Iterable

from sqlalchemy import func, select

from app.db.base import Base
from app.db.session import AsyncSessionLocal, engine
from app.models.document_base import Document
from app.models.document import DocumentSection
from app.models.section_dependency import SectionDependency
from app.models.history import EditHistory
from app.models.query import Query
from app.models.suggestion import EditSuggestion
from app.services.document_service import DocumentService
from app.services.search_service import SearchService

logger = logging.getLogger(__name__)


async def create_schema() -> None:
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)


def find_markdown_files(base_path: Path) -> list[Path]:
    files = list(base_path.glob("**/*.md"))
    if not files:
        raise FileNotFoundError(f"No markdown files found in {base_path}")
    return files



async def process_markdown_file(
    service: DocumentService,
    db,
    base_path: Path,
    file_path: Path,
) -> str:
    content = file_path.read_text(encoding="utf-8").strip()
    if not content:
        logger.warning("Skipping empty file: %s", file_path)
        return "skipped"

    relative_path = str(file_path.relative_to(base_path))
    existing = await service.get_document_by_path(relative_path)

    if existing:
        checksum = service.calculate_checksum(content)
        if checksum != existing.checksum:
            await service.update_document(relative_path, content)
            return "updated"
        return "unchanged"

    await service.create_document(
        file_path=relative_path,
        content=content,
        generate_embeddings=True,
    )
    return "created"


async def seed_documents(base_path: Path) -> None:
    await create_schema()

    md_files = find_markdown_files(base_path)

    stats = {"created": 0, "updated": 0, "unchanged": 0, "skipped": 0, "errors": 0}

    async with AsyncSessionLocal() as db:
        service = DocumentService(db)

        for md_file in md_files:
            try:
                result = await process_markdown_file(service, db, base_path, md_file)
                stats[result] += 1
                await db.commit()
            except Exception:
                stats["errors"] += 1
                await db.rollback()
                logger.exception("Failed processing %s", md_file)


async def clear_database() -> None:
    async with AsyncSessionLocal() as db:
        await db.execute(EditHistory.__table__.delete())
        await db.execute(EditSuggestion.__table__.delete())
        await db.execute(Query.__table__.delete())
        await db.execute(SectionDependency.__table__.delete())
        await db.execute(DocumentSection.__table__.delete())
        await db.execute(Document.__table__.delete())
        await db.commit()


async def clear_vectors(search: SearchService) -> None:
    try:
        count = search.get_collection_stats().get("count", 0)
        logger.info("Clearing %d vectors", count)
        search.clear_collection()
    except Exception:
        logger.warning("Failed to clear vector store", exc_info=True)


async def clear_and_reseed(base_path: Path) -> None:
    await create_schema()
    await clear_vectors(SearchService())
    await clear_database()
    await seed_documents(base_path)



async def show_stats() -> None:
    async with AsyncSessionLocal() as db:
        docs = await db.scalar(select(func.count(Document.id)))
        sections = await db.scalar(select(func.count(DocumentSection.id)))

    vectors = SearchService().get_collection_stats().get("count", 0)

    logger.info(
        "\nDocuments: %d\nSections: %d\nVectors: %d",
        docs,
        sections,
        vectors,
    )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Document database utilities")
    parser.add_argument("--path", type=Path, help="Base path for markdown files")
    parser.add_argument("--clear", action="store_true", help="Clear DB and reseed")
    parser.add_argument("--stats", action="store_true", help="Show database stats")
    return parser.parse_args()


def main() -> None:
    logging.basicConfig(level=logging.INFO)

    args = parse_args()
    base_path = args.path or Path("data/openai-agents-sdk")

    if args.stats:
        asyncio.run(show_stats())
    elif args.clear:
        asyncio.run(clear_and_reseed(base_path))
    else:
        asyncio.run(seed_documents(base_path))


if __name__ == "__main__":
    main()
